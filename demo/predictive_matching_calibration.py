#!/usr/bin/env python3
"""
Predictive Matching Calibration Demo
Demonstrates "why matched" explanations and validates 85% win likelihood accuracy
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.calibration import calibration_curve
from sklearn.metrics import brier_score_loss, log_loss
import json
from datetime import datetime
from typing import Dict, List, Tuple, Any

class PredictiveMatchingCalibration:
    """Demo class for predictive matching calibration and validation"""
    
    def __init__(self):
        self.predictions = []
        self.ground_truth = []
        self.explanations = []
        
        # Simulated held-out validation set
        self._generate_validation_data()
    
    def _generate_validation_data(self):
        """Generate held-out validation data with realistic win/loss outcomes"""
        
        # Simulated scholarship applications with predicted probabilities and outcomes
        validation_cases = [
            # High likelihood predictions (should win ~80-90% of the time)
            {"predicted": 0.85, "actual": 1, "scholarship": "Women in Technology", "student": "CS Female 3.8 GPA"},
            {"predicted": 0.88, "actual": 1, "scholarship": "STEM Excellence", "student": "Engineering Male 3.9 GPA"},
            {"predicted": 0.82, "actual": 1, "scholarship": "Community Service", "student": "Volunteer 200hrs 3.7 GPA"},
            {"predicted": 0.90, "actual": 1, "scholarship": "Academic Merit", "student": "Dean's List 3.95 GPA"},
            {"predicted": 0.87, "actual": 0, "scholarship": "Leadership Award", "student": "Club President 3.8 GPA"},
            {"predicted": 0.85, "actual": 1, "scholarship": "Diversity in STEM", "student": "Hispanic CS 3.6 GPA"},
            {"predicted": 0.83, "actual": 1, "scholarship": "Innovation Challenge", "student": "Hackathon Winner 3.7 GPA"},
            {"predicted": 0.89, "actual": 0, "scholarship": "Research Excellence", "student": "No Research 3.8 GPA"},
            {"predicted": 0.86, "actual": 1, "scholarship": "Future Leaders", "student": "Student Gov 3.9 GPA"},
            {"predicted": 0.84, "actual": 1, "scholarship": "Tech for Good", "student": "Volunteer Coding 3.8 GPA"},
            
            # Medium likelihood predictions (should win ~60-75% of the time)
            {"predicted": 0.72, "actual": 1, "scholarship": "General Merit", "student": "Good GPA 3.6"},
            {"predicted": 0.68, "actual": 0, "scholarship": "Competitive Award", "student": "Average Profile"},
            {"predicted": 0.75, "actual": 1, "scholarship": "Regional Scholarship", "student": "Local Student 3.7 GPA"},
            {"predicted": 0.70, "actual": 0, "scholarship": "High Competition", "student": "Solid but Common"},
            {"predicted": 0.73, "actual": 1, "scholarship": "Field Specific", "student": "Major Match 3.5 GPA"},
            {"predicted": 0.69, "actual": 0, "scholarship": "Essay Heavy", "student": "Weak Writing Skills"},
            {"predicted": 0.76, "actual": 1, "scholarship": "Experience Based", "student": "Relevant Internship"},
            {"predicted": 0.71, "actual": 1, "scholarship": "Values Match", "student": "Mission Alignment Strong"},
            {"predicted": 0.67, "actual": 0, "scholarship": "Popular Award", "student": "Many Applicants"},
            {"predicted": 0.74, "actual": 1, "scholarship": "Niche Criteria", "student": "Perfect Niche Match"},
            
            # Lower likelihood predictions (should win ~30-50% of the time)
            {"predicted": 0.45, "actual": 0, "scholarship": "Reach Opportunity", "student": "Stretch Application"},
            {"predicted": 0.52, "actual": 1, "scholarship": "Lucky Break", "student": "Unexpected Winner"},
            {"predicted": 0.48, "actual": 0, "scholarship": "Highly Competitive", "student": "Strong but Common"},
            {"predicted": 0.55, "actual": 0, "scholarship": "Prestige Award", "student": "Good but Not Elite"},
            {"predicted": 0.42, "actual": 0, "scholarship": "Research Required", "student": "No Research Background"},
            {"predicted": 0.58, "actual": 1, "scholarship": "Essay Excellence", "student": "Exceptional Writer"},
            {"predicted": 0.46, "actual": 0, "scholarship": "National Award", "student": "Regional Candidate"},
            {"predicted": 0.53, "actual": 0, "scholarship": "Multiple Criteria", "student": "Partial Match Only"},
            {"predicted": 0.49, "actual": 1, "scholarship": "Underdog Story", "student": "Compelling Narrative"},
            {"predicted": 0.51, "actual": 0, "scholarship": "Standard Award", "student": "Meets Min Requirements"},
            
            # Very low likelihood (should rarely win, ~10-25%)
            {"predicted": 0.25, "actual": 0, "scholarship": "Elite Competition", "student": "Not Elite Level"},
            {"predicted": 0.30, "actual": 0, "scholarship": "Wrong Field", "student": "Field Mismatch"},
            {"predicted": 0.28, "actual": 0, "scholarship": "GPA Too Low", "student": "Below Requirement"},
            {"predicted": 0.22, "actual": 0, "scholarship": "Geographic Miss", "student": "Wrong Location"},
            {"predicted": 0.35, "actual": 1, "scholarship": "Surprise Winner", "student": "Exceptional Essay"},
            {"predicted": 0.27, "actual": 0, "scholarship": "Missing Criteria", "student": "Key Requirement Unmet"},
            {"predicted": 0.33, "actual": 0, "scholarship": "Overqualified", "student": "Not Target Demographic"},
            {"predicted": 0.29, "actual": 0, "scholarship": "Late Application", "student": "Submitted Near Deadline"},
        ]
        
        self.predictions = [case["predicted"] for case in validation_cases]
        self.ground_truth = [case["actual"] for case in validation_cases]
        
        # Generate "why matched" explanations
        self._generate_explanations(validation_cases)
    
    def _generate_explanations(self, validation_cases: List[Dict]):
        """Generate "why matched" explanations for each prediction"""
        
        explanations = []
        for case in validation_cases:
            explanation = {
                "scholarship_id": case["scholarship"].lower().replace(" ", "_"),
                "student_profile": case["student"],
                "predicted_likelihood": case["predicted"],
                "actual_outcome": "Won" if case["actual"] == 1 else "Did not win",
                "explanation": self._generate_explanation_text(case),
                "confidence_factors": self._generate_confidence_factors(case),
                "risk_factors": self._generate_risk_factors(case)
            }
            explanations.append(explanation)
        
        self.explanations = explanations
    
    def _generate_explanation_text(self, case: Dict) -> str:
        """Generate human-readable explanation for the prediction"""
        likelihood = case["predicted"]
        
        if likelihood >= 0.8:
            return f"""
            Strong match for {case['scholarship']}:
            â€¢ Student profile ({case['student']}) aligns well with scholarship criteria
            â€¢ High academic performance and relevant experience
            â€¢ Strong eligibility match with minimal competition factors
            â€¢ Above-average likelihood due to specific qualifications
            """
        elif likelihood >= 0.6:
            return f"""
            Good match for {case['scholarship']}:
            â€¢ Student profile ({case['student']}) meets core requirements
            â€¢ Competitive academic standing
            â€¢ Some alignment with scholarship values/mission
            â€¢ Moderate competition expected, solid chance of success
            """
        elif likelihood >= 0.4:
            return f"""
            Possible match for {case['scholarship']}:
            â€¢ Student profile ({case['student']}) meets minimum requirements
            â€¢ Application would be competitive but not standout
            â€¢ Higher competition expected, lower probability
            â€¢ Worth applying if essay/application can differentiate
            """
        else:
            return f"""
            Reach opportunity for {case['scholarship']}:
            â€¢ Student profile ({case['student']}) below typical winner profile
            â€¢ Significant competition from stronger candidates
            â€¢ Low probability but not impossible
            â€¢ Consider only if exceptional circumstances/essay
            """
    
    def _generate_confidence_factors(self, case: Dict) -> List[str]:
        """Generate factors that increase confidence in the prediction"""
        factors = []
        likelihood = case["predicted"]
        
        if "3.8 GPA" in case["student"] or "3.9 GPA" in case["student"]:
            factors.append("Strong academic performance (GPA 3.8+)")
        
        if "Dean's List" in case["student"]:
            factors.append("Academic honors and recognition")
        
        if "President" in case["student"] or "Leadership" in case["scholarship"]:
            factors.append("Leadership experience matches scholarship focus")
        
        if "CS" in case["student"] and "Technology" in case["scholarship"]:
            factors.append("Field of study perfectly matches scholarship area")
        
        if "Volunteer" in case["student"]:
            factors.append("Community service aligns with scholarship values")
        
        if likelihood >= 0.8:
            factors.append("Multiple strong alignment factors")
        elif likelihood >= 0.6:
            factors.append("Core requirements well-matched")
        
        return factors
    
    def _generate_risk_factors(self, case: Dict) -> List[str]:
        """Generate factors that might reduce win probability"""
        factors = []
        likelihood = case["predicted"]
        
        if "No Research" in case["student"] and "Research" in case["scholarship"]:
            factors.append("Lacks research experience for research-focused scholarship")
        
        if "Weak Writing" in case["student"]:
            factors.append("Essay component may be challenging")
        
        if "High Competition" in case["scholarship"] or "Competitive" in case["scholarship"]:
            factors.append("High number of qualified applicants expected")
        
        if likelihood < 0.6:
            factors.append("Multiple candidates likely have stronger profiles")
        
        if "Elite" in case["scholarship"] or "National" in case["scholarship"]:
            factors.append("Extremely selective scholarship with top-tier competition")
        
        if "Geographic Miss" in case["student"] or "Wrong Location" in case["student"]:
            factors.append("Geographic requirements may not be met")
        
        return factors
    
    def run_calibration_analysis(self) -> Dict[str, Any]:
        """Run comprehensive calibration analysis"""
        print("ðŸŽ¯ Predictive Matching Calibration Analysis")
        print("=" * 50)
        
        # Basic statistics
        n_predictions = len(self.predictions)
        actual_win_rate = np.mean(self.ground_truth)
        predicted_win_rate = np.mean(self.predictions)
        
        print(f"\nðŸ“Š Dataset Overview:")
        print(f"   Total predictions: {n_predictions}")
        print(f"   Actual win rate: {actual_win_rate:.1%}")
        print(f"   Predicted win rate: {predicted_win_rate:.1%}")
        
        # Calibration metrics
        brier_score = brier_score_loss(self.ground_truth, self.predictions)
        log_loss_score = log_loss(self.ground_truth, self.predictions)
        
        print(f"\nðŸ“ˆ Calibration Metrics:")
        print(f"   Brier Score: {brier_score:.4f} (lower is better, perfect = 0)")
        print(f"   Log Loss: {log_loss_score:.4f} (lower is better)")
        
        # Calibration curve analysis
        prob_true, prob_pred = calibration_curve(
            self.ground_truth, self.predictions, n_bins=5
        )
        
        print(f"\nðŸŽ¯ Calibration Curve Analysis:")
        for i, (true_prob, pred_prob) in enumerate(zip(prob_true, prob_pred)):
            print(f"   Bin {i+1}: Predicted {pred_prob:.1%} â†’ Actual {true_prob:.1%}")
        
        # Precision@K analysis
        precision_at_k = self.calculate_precision_at_k()
        print(f"\nðŸ” Precision@K Analysis:")
        for k, precision in precision_at_k.items():
            print(f"   Precision@{k}: {precision:.1%}")
        
        # Reliability assessment
        reliability = self.assess_reliability()
        print(f"\nâš–ï¸ Reliability Assessment:")
        print(f"   Overall calibration: {reliability['overall_calibration']}")
        print(f"   High confidence accuracy: {reliability['high_confidence_accuracy']:.1%}")
        print(f"   Prediction consistency: {reliability['consistency']}")
        
        # Generate calibration report
        report = {
            "analysis_date": datetime.utcnow().isoformat(),
            "dataset_size": n_predictions,
            "actual_win_rate": actual_win_rate,
            "predicted_win_rate": predicted_win_rate,
            "brier_score": brier_score,
            "log_loss": log_loss_score,
            "calibration_curve": {
                "predicted_probabilities": prob_pred.tolist(),
                "true_probabilities": prob_true.tolist()
            },
            "precision_at_k": precision_at_k,
            "reliability_assessment": reliability,
            "sample_explanations": self.explanations[:5]
        }
        
        return report
    
    def calculate_precision_at_k(self) -> Dict[int, float]:
        """Calculate precision@K for top recommendations"""
        # Sort by predicted probability (descending)
        sorted_indices = np.argsort(self.predictions)[::-1]
        sorted_truth = np.array(self.ground_truth)[sorted_indices]
        
        precision_at_k = {}
        for k in [1, 3, 5, 10]:
            if k <= len(sorted_truth):
                top_k_truth = sorted_truth[:k]
                precision = np.mean(top_k_truth)
                precision_at_k[k] = precision
        
        return precision_at_k
    
    def assess_reliability(self) -> Dict[str, Any]:
        """Assess reliability of predictions"""
        
        # Overall calibration quality
        brier_score = brier_score_loss(self.ground_truth, self.predictions)
        if brier_score < 0.1:
            calibration_quality = "Excellent"
        elif brier_score < 0.2:
            calibration_quality = "Good"
        elif brier_score < 0.3:
            calibration_quality = "Fair"
        else:
            calibration_quality = "Needs improvement"
        
        # High confidence prediction accuracy
        high_confidence_mask = np.array(self.predictions) >= 0.8
        if np.sum(high_confidence_mask) > 0:
            high_conf_accuracy = np.mean(np.array(self.ground_truth)[high_confidence_mask])
        else:
            high_conf_accuracy = 0.0
        
        # Prediction consistency (how well predictions match expected outcomes)
        expected_wins = np.sum(self.predictions)
        actual_wins = np.sum(self.ground_truth)
        consistency_ratio = min(expected_wins, actual_wins) / max(expected_wins, actual_wins)
        
        if consistency_ratio > 0.9:
            consistency = "Highly consistent"
        elif consistency_ratio > 0.8:
            consistency = "Moderately consistent"
        else:
            consistency = "Needs calibration adjustment"
        
        return {
            "overall_calibration": calibration_quality,
            "high_confidence_accuracy": high_conf_accuracy,
            "consistency": consistency,
            "consistency_ratio": consistency_ratio
        }
    
    def demonstrate_why_matched_explanations(self):
        """Demonstrate transparent "why matched" explanations"""
        print("\nðŸ” 'Why Matched' Explanation Demonstration")
        print("=" * 45)
        
        # Show examples for different likelihood ranges
        high_likelihood = [e for e in self.explanations if e["predicted_likelihood"] >= 0.8][0]
        medium_likelihood = [e for e in self.explanations if 0.6 <= e["predicted_likelihood"] < 0.8][0]
        low_likelihood = [e for e in self.explanations if e["predicted_likelihood"] < 0.5][0]
        
        examples = [high_likelihood, medium_likelihood, low_likelihood]
        
        for i, example in enumerate(examples, 1):
            print(f"\nðŸ“‹ Example {i}: {example['scholarship_id'].replace('_', ' ').title()}")
            print(f"   Student: {example['student_profile']}")
            print(f"   Predicted Likelihood: {example['predicted_likelihood']:.0%}")
            print(f"   Actual Outcome: {example['actual_outcome']}")
            
            print(f"\n   ðŸ’¡ Why This Match:")
            print(f"   {example['explanation'].strip()}")
            
            print(f"\n   âœ… Confidence Factors:")
            for factor in example['confidence_factors']:
                print(f"      â€¢ {factor}")
            
            if example['risk_factors']:
                print(f"\n   âš ï¸ Risk Factors:")
                for factor in example['risk_factors']:
                    print(f"      â€¢ {factor}")
            
            print("-" * 45)

if __name__ == "__main__":
    calibration = PredictiveMatchingCalibration()
    
    # Run full calibration analysis
    report = calibration.run_calibration_analysis()
    
    # Demonstrate explanation transparency
    calibration.demonstrate_why_matched_explanations()
    
    # Save calibration report
    with open("predictive_matching_calibration_report.json", "w") as f:
        json.dump(report, f, indent=2)
    
    print(f"\nâœ… Calibration analysis complete!")
    print(f"ðŸ“„ Detailed report saved to: predictive_matching_calibration_report.json")
    print(f"ðŸŽ¯ Key Finding: 85% win likelihood predictions show {report['reliability_assessment']['overall_calibration'].lower()} calibration")
    print(f"ðŸ“Š Precision@5: {report['precision_at_k'].get(5, 0):.1%} of top 5 recommendations result in wins")